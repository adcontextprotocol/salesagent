"""
Delivery Webhook Scheduler

Sends daily delivery reports via webhooks for media buys that have configured reporting_webhook.
This runs as a background task and sends reports when GAM data is fresh (after 4 AM PT daily).
"""

import asyncio
import logging
from datetime import UTC, datetime, timedelta
from typing import Any

import pytz
from sqlalchemy import select

from src.core.database.database_session import get_db_session
from src.core.database.models import MediaBuy
from src.core.database.models import PushNotificationConfig as DBPushNotificationConfig
from src.core.schemas import GetMediaBuyDeliveryRequest
from src.core.tools.media_buy_delivery import _get_media_buy_delivery_impl
from src.services.protocol_webhook_service import get_protocol_webhook_service

logger = logging.getLogger(__name__)


class DeliveryWebhookScheduler:
    """Scheduler for sending daily delivery reports via webhooks."""

    def __init__(self):
        self.webhook_service = get_protocol_webhook_service()
        self.is_running = False
        self._task: asyncio.Task | None = None
        self._lock = asyncio.Lock()

    async def start(self):
        """Start the scheduler background task."""
        async with self._lock:
            if self.is_running:
                logger.warning("Delivery webhook scheduler is already running")
                return

            self.is_running = True
            self._task = asyncio.create_task(self._run_scheduler())
            logger.info("Delivery webhook scheduler started")

    async def stop(self):
        """Stop the scheduler background task."""
        async with self._lock:
            if not self.is_running:
                return

            self.is_running = False
            if self._task:
                self._task.cancel()
                try:
                    await self._task
                except asyncio.CancelledError:
                    pass
            logger.info("Delivery webhook scheduler stopped")

    async def _run_scheduler(self):
        """Main scheduler loop - runs daily after GAM data is fresh.

        GAM data freezes at 3 AM PT. We run at 4 AM PT to ensure:
        - Previous day's data is finalized (3 AM PT freeze)
        - 1 hour buffer for processing
        - Still early enough for morning reports
        - Handles PST/PDT transitions automatically
        """
        while self.is_running:
            try:
                # Calculate next run time using Pacific timezone (handles PST/PDT automatically)
                pt_tz = pytz.timezone("America/Los_Angeles")
                now_pt = datetime.now(pt_tz)

                # Schedule for 4 AM PT (1 hour after 3 AM PT GAM freeze)
                next_run_pt = now_pt.replace(hour=4, minute=0, second=0, microsecond=0)

                # If we've passed today's run time, schedule for tomorrow
                if now_pt >= next_run_pt:
                    next_run_pt += timedelta(days=1)

                # Convert to UTC for sleep calculation
                next_run = next_run_pt.astimezone(pytz.UTC)
                now = datetime.now(pytz.UTC)

                # Wait until next run time
                wait_seconds = (next_run - now).total_seconds()
                logger.info(
                    f"Next delivery webhook run scheduled for {next_run} UTC (in {wait_seconds/3600:.1f} hours)"
                )

                await asyncio.sleep(wait_seconds)

                # Send reports for all active media buys with webhooks
                await self._send_daily_reports()

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in delivery webhook scheduler: {e}", exc_info=True)
                # Wait 1 hour before retrying on error
                await asyncio.sleep(3600)

    async def _send_daily_reports(self):
        """Send delivery reports for all active media buys with configured webhooks."""
        logger.info("Starting daily delivery report webhook batch")

        try:
            # Step 1: Extract data from database (quick, synchronous)
            media_buy_data = []
            with get_db_session() as session:
                # Find all active media buys
                stmt = select(MediaBuy).where(MediaBuy.status.in_(["active", "approved"]))
                media_buys = session.scalars(stmt).all()

                for media_buy in media_buys:
                    # Check if this media buy has a reporting webhook configured
                    raw_request = media_buy.raw_request or {}
                    reporting_webhook = raw_request.get("reporting_webhook")

                    if not reporting_webhook:
                        continue

                    # Extract data needed for webhook (detach from session)
                    media_buy_data.append(
                        {
                            "media_buy_id": media_buy.media_buy_id,
                            "tenant_id": media_buy.tenant_id,
                            "principal_id": media_buy.principal_id,
                            "status": media_buy.status,
                            "currency": media_buy.currency,
                            "start_time": media_buy.start_time,
                            "end_time": media_buy.end_time,
                            "reporting_webhook": reporting_webhook,
                        }
                    )

            # Step 2: Send reports asynchronously (DB session already closed)
            reports_sent = 0
            errors = 0

            for data in media_buy_data:
                try:
                    await self._send_report_for_media_buy_data(data)
                    reports_sent += 1

                except Exception as e:
                    logger.error(f"Error sending report for media buy {data['media_buy_id']}: {e}", exc_info=True)
                    errors += 1

            logger.info(f"Daily delivery report batch complete: {reports_sent} sent, {errors} errors")

        except Exception as e:
            logger.error(f"Error in daily delivery report batch: {e}", exc_info=True)

    async def _send_report_for_media_buy_data(self, data: dict[str, Any]):
        """Send a delivery report for a single media buy using extracted data.

        Args:
            data: Dictionary containing media_buy_id, tenant_id, principal_id, and reporting_webhook
        """
        media_buy_id = data["media_buy_id"]
        tenant_id = data["tenant_id"]
        principal_id = data["principal_id"]
        reporting_webhook = data["reporting_webhook"]

        try:
            # Calculate reporting period (yesterday's data)
            yesterday = datetime.now(UTC) - timedelta(days=1)
            start_date = yesterday.replace(hour=0, minute=0, second=0, microsecond=0)
            end_date = yesterday.replace(hour=23, minute=59, second=59, microsecond=0)

            # Fetch delivery metrics
            # Create a minimal context object for the delivery call
            import uuid

            from src.core.tool_context import ToolContext

            context = ToolContext(
                context_id=str(uuid.uuid4()),
                tenant_id=tenant_id,
                principal_id=principal_id,
                tool_name="get_media_buy_delivery",
                request_timestamp=datetime.now(UTC),
            )

            req = GetMediaBuyDeliveryRequest(  # type: ignore[call-arg]
                media_buy_ids=[media_buy_id],
                start_date=start_date.strftime("%Y-%m-%d"),
                end_date=end_date.strftime("%Y-%m-%d"),
            )

            delivery_response = _get_media_buy_delivery_impl(req, context)

            # Validate data freshness before sending webhook (optional - only if reporting_data available)
            # Skip webhook if data isn't fresh enough
            if hasattr(delivery_response, "reporting_data") and delivery_response.reporting_data:
                try:
                    from src.adapters.gam_data_freshness import validate_and_log_freshness

                    is_fresh = validate_and_log_freshness(
                        delivery_response.reporting_data, media_buy_id, target_date=yesterday
                    )
                    if not is_fresh:
                        logger.warning(f"Skipping webhook for {media_buy_id} - data not fresh enough")
                        return
                except Exception as e:
                    # If freshness check fails, log warning but continue (data freshness is optional)
                    logger.warning(f"Could not validate data freshness for {media_buy_id}: {e}")

            # Get sequence number for this webhook (requires new DB session)
            sequence_number = 1
            with get_db_session() as session:
                from sqlalchemy import func, select

                from src.core.database.models import WebhookDeliveryLog

                try:
                    stmt = select(func.coalesce(func.max(WebhookDeliveryLog.sequence_number), 0)).where(
                        WebhookDeliveryLog.media_buy_id == media_buy_id,
                        WebhookDeliveryLog.task_type == "delivery_report",
                    )
                    max_seq = session.scalar(stmt)
                    sequence_number = (max_seq or 0) + 1
                except Exception as e:
                    logger.warning(f"Could not get sequence number for media buy {media_buy_id}: {e}")

            # Calculate next_expected_at (24 hours from now at 4 AM PT)
            import pytz

            pt_tz = pytz.timezone("America/Los_Angeles")
            now_pt = datetime.now(pt_tz)
            next_run_pt = now_pt.replace(hour=4, minute=0, second=0, microsecond=0) + timedelta(days=1)
            next_expected_at = next_run_pt.astimezone(pytz.UTC).isoformat()

            # Add webhook-specific metadata to the response
            response_dict = delivery_response.model_dump()
            response_dict.update(
                {
                    "notification_type": "scheduled",
                    "sequence_number": sequence_number,
                    "next_expected_at": next_expected_at,
                    "partial_data": False,  # TODO: Check for reporting_delayed status in media_buy_deliveries
                    "unavailable_count": 0,  # TODO: Count reporting_delayed/failed deliveries
                }
            )

            # Extract webhook URL and authentication
            webhook_url = reporting_webhook.get("url")
            if not webhook_url:
                logger.warning(f"No webhook URL configured for media buy {media_buy_id}")
                return

            # Try to find existing push notification config or create a temporary one
            auth_config = reporting_webhook.get("authentication", {})
            auth_type = None
            auth_token = None

            if auth_config:
                schemes = auth_config.get("schemes", [])
                auth_type = schemes[0] if schemes else None
                auth_token = auth_config.get("credentials")

            # Query for existing push notification config (short-lived session)
            webhook_config = None
            with get_db_session() as session:
                config_stmt = select(DBPushNotificationConfig).where(
                    DBPushNotificationConfig.principal_id == principal_id,
                    DBPushNotificationConfig.tenant_id == tenant_id,
                    DBPushNotificationConfig.url == webhook_url,
                    DBPushNotificationConfig.is_active,
                )
                push_config = session.scalars(config_stmt).first()

                # Extract webhook config data before session closes
                if push_config:
                    # Create a detached config object with the data
                    webhook_config = DBPushNotificationConfig(
                        id=push_config.id,
                        url=push_config.url,
                        authentication_type=push_config.authentication_type,
                        authentication_token=push_config.authentication_token,
                        tenant_id=push_config.tenant_id,
                        principal_id=push_config.principal_id,
                        is_active=push_config.is_active,
                    )

            if not webhook_config:
                # Create a temporary config (not persisted to database)
                webhook_config = DBPushNotificationConfig(
                    id=f"temp_{media_buy_id}",
                    url=webhook_url,
                    authentication_type=auth_type,
                    authentication_token=auth_token,
                    tenant_id=tenant_id,
                    principal_id=principal_id,
                    is_active=True,
                )

            # Send webhook (async, no DB session needed)
            await self.webhook_service.send_notification(
                task_type="delivery_report",
                task_id=media_buy_id,
                status="completed",
                push_notification_config=webhook_config,
                result=response_dict,  # Use modified dict with webhook metadata
                tenant_id=tenant_id,
                principal_id=principal_id,
                media_buy_id=media_buy_id,
                notification_type="scheduled",  # Daily scheduled delivery report
            )

            logger.info(f"Sent delivery report webhook for media buy {media_buy_id}")

        except Exception as e:
            logger.error(f"Error sending delivery report for media buy {media_buy_id}: {e}", exc_info=True)
            raise


# Global scheduler instance
_scheduler: DeliveryWebhookScheduler | None = None


def get_delivery_webhook_scheduler() -> DeliveryWebhookScheduler:
    """Get or create global scheduler instance."""
    global _scheduler
    if _scheduler is None:
        _scheduler = DeliveryWebhookScheduler()
    return _scheduler


async def start_delivery_webhook_scheduler():
    """Start the delivery webhook scheduler (called at application startup)."""
    scheduler = get_delivery_webhook_scheduler()
    await scheduler.start()


async def stop_delivery_webhook_scheduler():
    """Stop the delivery webhook scheduler (called at application shutdown)."""
    scheduler = get_delivery_webhook_scheduler()
    await scheduler.stop()
