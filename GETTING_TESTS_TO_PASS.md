# Getting Tests to Pass - Implementation Guide

## Current State
We've made significant improvements to the test suite architecture, but tests still need work to pass consistently. Here's what's been done and what's needed.

## ‚úÖ Completed Improvements

### 1. Test Suite Quality
- Removed all 29 `@pytest.mark.skip` decorators
- Reduced test count from 60 to 35 files (41.7% reduction)
- Created comprehensive smoke test suite in `tests/smoke/`
- Added pre-commit hooks to prevent test skipping

### 2. Infrastructure Fixes
- Fixed `main.py` module-level database initialization (checks `PYTEST_CURRENT_TEST`)
- Created database fixtures in `tests/conftest_db.py`
- Added `run_tests.sh` script for easy test execution
- Updated CI pipeline with smoke test job

## ‚ùå Remaining Issues to Fix

### 1. Server Dependencies (High Priority)
Many integration tests expect running servers but don't start them:
- MCP server on port 8080
- Admin UI on port 8001

**Solution Options:**
```python
# Option A: Add fixtures to start/stop servers
@pytest.fixture(scope="session")
def mcp_server():
    """Start MCP server for tests."""
    process = subprocess.Popen(["python", "main.py"])
    time.sleep(2)  # Wait for startup
    yield
    process.terminate()

# Option B: Mock server calls in integration tests
@pytest.fixture
def mock_mcp_client():
    """Provide mocked MCP client."""
    # Return mock client that doesn't need server
```

### 2. Database State Issues (Medium Priority)
Tests fail due to missing data or incorrect database state:
- Some tests expect specific tenants/principals
- Migration tests fail on missing `migrate.py` script
- Table creation order issues with foreign keys

**Solution:**
```python
# Ensure proper test data setup
@pytest.fixture(autouse=True)
def setup_test_data(db_session):
    """Create standard test data."""
    # Create test tenant
    # Create test principal
    # Create test products
```

### 3. Import-Time Side Effects (Medium Priority)
Several modules perform actions at import time:
- Database connections
- Config loading
- API client initialization

**Solution:**
```python
# Wrap initialization in functions
def initialize_app():
    """Initialize app components."""
    init_db()
    load_config()

# Only call in production
if __name__ == "__main__":
    initialize_app()
```

### 4. Environment Variable Dependencies (Low Priority)
Tests fail when environment variables aren't set:
- `GEMINI_API_KEY`
- `GOOGLE_CLIENT_ID`
- OAuth credentials

**Solution:**
```bash
# Add to conftest.py
@pytest.fixture(autouse=True)
def set_test_env():
    """Set test environment variables."""
    os.environ.update({
        "GEMINI_API_KEY": "test_key",
        "GOOGLE_CLIENT_ID": "test_id",
        # etc.
    })
```

## üöÄ Quick Fixes to Get Tests Passing

### Step 1: Minimal Changes for CI
```bash
# 1. Update smoke tests to not require servers
# Replace server health checks with import tests

# 2. Mark server-dependent tests
@pytest.mark.requires_server
def test_mcp_endpoint():
    ...

# 3. Skip in CI
pytest -m "not requires_server"
```

### Step 2: Fix Database Tests
```python
# tests/conftest.py additions
@pytest.fixture(scope="session", autouse=True)
def setup_test_database():
    """Ensure database is ready for all tests."""
    from models import Base
    from database_session import get_engine

    engine = get_engine()
    Base.metadata.create_all(engine)
```

### Step 3: Mock External Dependencies
```python
# tests/conftest.py
@pytest.fixture(autouse=True)
def mock_external_apis():
    """Mock all external API calls."""
    with patch("adapters.google_ad_manager.GoogleAdManager._api_call"):
        with patch("ai_product_creator.generate_product_config"):
            yield
```

## üìä Test Execution Strategy

### Local Development
```bash
# Quick smoke test
./run_tests.sh smoke

# Fast iteration
./run_tests.sh quick

# Full suite
./run_tests.sh all
```

### CI Pipeline
```yaml
jobs:
  smoke-tests:
    # Always run first - critical paths only
    # Should ALWAYS pass

  unit-tests:
    # No external dependencies
    # Should pass with mocks

  integration-tests:
    # Database required
    # Skip server-dependent tests

  e2e-tests:
    # Optional - can fail
    # Requires full environment
```

## üéØ Priority Order

1. **Get smoke tests passing** (1-2 hours)
   - Fix import issues
   - Remove server dependencies
   - Ensure database fixtures work

2. **Get unit tests passing** (2-3 hours)
   - Add missing mocks
   - Fix import-time side effects
   - Ensure isolation

3. **Get integration tests passing** (3-4 hours)
   - Proper database setup
   - Mark server-dependent tests
   - Fix data dependencies

4. **Get e2e tests passing** (Optional)
   - Full environment setup
   - Real server startup
   - Complete data flow

## üí° Key Insights

1. **Tests were written assuming a running system** - Need fixtures to provide that environment or mocks to simulate it

2. **Database state management is critical** - Each test needs predictable starting state

3. **Import-time side effects break tests** - Need lazy initialization pattern

4. **Server dependencies should be optional** - Use marks to skip when servers aren't available

## Next Immediate Actions

```bash
# 1. Fix the most basic tests first
pytest tests/smoke/test_smoke_basic.py -xvs

# 2. Identify specific failures
pytest --tb=short --co -q

# 3. Add fixtures for common failures
# Edit tests/conftest.py to add needed fixtures

# 4. Mark flaky tests for later
@pytest.mark.flaky
def test_sometimes_fails():
    ...
```

The goal is incremental progress: **Get SOME tests passing**, then gradually expand coverage.
